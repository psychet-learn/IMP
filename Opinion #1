Opinion #1

"""
현재 우리 프로젝트에서 음악에 관한 데이터는 음, 옥타브, 세기, 속도로 총 4가지 요소로 받고 있음.
그러나 거의 대부분 모든 AI기반 음악학습모델은 세기, 속도가 빠진 음으로만 이루어진 음악임.
그래서 인간의 작곡과정을 진지하게 고민해보고 우리 프로젝트의 데이터를 본 결과 조금만 수정하면 좋을 것 같은부분이 보였음.
먼저 차원을 축소하면 LOSS함수의 극솟값을 찾기 수월하므로 기존의 음의 클래스 12개와 옥타브를 분리해서 데이터를 다루는 것이 아닌
음과 옥타브를 합쳐서 총 88개 클래스의 음을 다루는 것이 좋을 것 같다는 생각이 들었음.(어차피 옥타브가 다른 같음 음 ex) A5 = A6 도 다른 음이니까) 
그리고 속도와 세기같은 부분은 한 시점에서의 음이 아닌 전체적인 맥락에서 결정되는 경우가 다분하므로(기교의 성격이 강하기 때문) 먼저 88개 클래스의 
음 데이터로 학습해서 모델의 정확도를 높힌 뒤 이 모델을 기반으로 속도와 세기를 추가하는 방식으로 모델을 구성해도 괜찮을 것 같음. 

 작곡을 하나도 모르는 인간이 작곡을 배울 때, 먼저 음을 배우고 코드를 배우고 화음을 넣은 후에 곡 분위기에 따라 속도와 세기를  생각하는 것 처럼 
딥러닝도 그렇게 하면 좋지 않을까 하는 생각이 듦. (물론 더 자세하고 정확한 과정은 차후 작곡하는 음악학원원장님께 자문을 구해볼 예정)

그럼 주말에도 건실하게 살아봅시다!
"""
